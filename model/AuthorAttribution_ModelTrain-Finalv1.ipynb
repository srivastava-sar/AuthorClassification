{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def cleanData(text):\n",
    "    removeList = [\"@handle\",\"@handle:\"]\n",
    "    text_r = \" \".join([char for char in text.split(' ') if char not in removeList and not any(i.isdigit() for i in char) and char.isdigit]) \n",
    "    return text_r\n",
    "\n",
    "def removeHyperLink(text):\n",
    "    text,count = re.subn(r'http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return(text,count)\n",
    "    \n",
    "def removeSpChar(text):\n",
    "    new = re.sub('[@_!#$%^&*()<>?/\\|}{~:\\']' ,'', text)\n",
    "    count = len(text) - len(new)\n",
    "    return new,count\n",
    "\n",
    "def removeRT(text):\n",
    "    text_r = \" \".join([char for char in text.split(' ') if char != 'RT'])\n",
    "    return text_r\n",
    "\n",
    "\n",
    "##Normalization\n",
    "\n",
    "# def normalizeDF(df_orig):\n",
    "#     normalizeColumn(df_orig,'chCount')\n",
    "#     normalizeColumn(df_orig,'wdCount')\n",
    "#     normalizeColumn(df_orig,'spCount')\n",
    "#     #normalizeColumn(df_orig,'isCap')\n",
    "#     normalizeColumn(df_orig,'hashtags')\n",
    "#     return df_orig\n",
    "\n",
    "    \n",
    "## Tokenize and stem data\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        #if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "    stems = []\n",
    "    for item in filtered_sentence:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return ' '.join(word for word in stems)\n",
    "\n",
    "def preprocessingDF(df_orig):\n",
    "    df_orig['tweet'] = df_orig['rtweet'].apply(lambda x: cleanData(x))\n",
    "    df_orig['tweet'],df_orig['NumLink'] = zip(*df_orig['tweet'].map(removeHyperLink))\n",
    "    df_orig['hashtags'] = df_orig['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    df_orig['tweet'],df_orig['spCount'] =  zip(*df_orig['tweet'].map(removeSpChar))\n",
    "    df_orig['wdCount'] = df_orig['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df_orig['chCount'] = df_orig['tweet'].str.len()\n",
    "    df_orig['isCap'] = df_orig['tweet'].apply(lambda x: len([x for x in x.split() if x[0].isupper()]))\n",
    "    df_orig['isRT'] = df_orig['tweet'].apply(lambda x: 1 if 'RT' in x else 0)\n",
    "    df_orig['tweet'] = df_orig['tweet'].apply(lambda x: removeRT(x))\n",
    "    df_orig['tweet'] = df_orig['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    df_orig['tweet'] = df_orig['tweet'].apply(lambda x: tokenize(x))\n",
    "    return df_orig\n",
    "\n",
    "\n",
    "def trainRT(df_orig):\n",
    "    ## Rate of RT\n",
    "    #df_orig['atweet'] = df_orig.groupby(\"id\")['tweet'].transform(lambda x: ' '.join(x))\n",
    "    df_orig['TotTw'] = df_orig.groupby(\"id\")['tweet'].transform('count')\n",
    "    df_orig['RTrate'] = df_orig.groupby(\"id\")['isRT'].transform('sum')\n",
    "    df_orig['RTrate'] = df_orig['RTrate']/df_orig['TotTw']\n",
    "    return df_orig\n",
    "\n",
    "def testRT(df_orig):\n",
    "    df_orig['RTrate'] = df_orig['isRT']\n",
    "    return df_orig\n",
    "\n",
    "def dropFeatures(df_orig):\n",
    "    return df_orig.drop(columns=[\"NumLink\",\"spCount\",\"hashtags\",\"chCount\",\"isCap\",\"isRT\",\"wdCount\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saransh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_original_tweet = pd.read_table('../data/train_tweets.txt')\n",
    "#df_original_tweet = preprocessing(df_original_tweet)\n",
    "\n",
    "#df_train_tweet = normalizeDF(df_train_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_original_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_tweet = df_original_tweet.iloc[:, 1:].values\n",
    "y = df_original_tweet.iloc[:,0].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# encoded_Y\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(df_tweet, encoded_Y, test_size=0.20,random_state=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(sentences_train,columns=[\"rtweet\"])\n",
    "df_train = preprocessingDF(df_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wdCount_scaler = preprocessing.MinMaxScaler()\n",
    "wdCount = df_train['wdCount'].values #returns a numpy array\n",
    "\n",
    "wdCount = wdCount.reshape(-1, 1)\n",
    "\n",
    "wdCount_scaler.fit(wdCount)\n",
    "wdCount_scaled = wdCount_scaler.transform(wdCount)\n",
    "df_train['wdCount'] = pd.DataFrame(wdCount_scaled)\n",
    "\n",
    "\n",
    "\n",
    "chCount_scaler = preprocessing.MinMaxScaler()\n",
    "chCount = df_train['chCount'].values #returns a numpy array\n",
    "chCount = chCount.reshape(-1, 1)\n",
    "\n",
    "chCount_scaler.fit(chCount)\n",
    "chCount_scaled = chCount_scaler.transform(chCount)\n",
    "df_train['chCount'] = pd.DataFrame(chCount_scaled)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_train = dropFeatures(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df_train['tweet'].values)\n",
    "\n",
    "X_bow_train = vectorizer.transform(df_train['tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<262555x92911 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2896777 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vecs = tfidf.fit(df_train['tweet'].values)\n",
    "\n",
    "X_tfidf_train = tfidf_vecs.transform(df_train['tweet'].values)\n",
    "X_tfidf_train\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_tfidf = TruncatedSVD(n_components=1000)\n",
    "svd_tfidf.fit(X_tfidf_train)\n",
    "\n",
    "matrix_tfidf_train_lowrank = svd_tfidf.transform(X_tfidf_train)\n",
    "\n",
    "#matrix_tfidf_train_lowrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_train = pd.DataFrame(matrix_tfidf_train_lowrank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "svd.fit(X_bow_train)\n",
    "\n",
    "matrix_train_lowrank = svd.transform(X_bow_train)\n",
    "\n",
    "df_bow_train = pd.DataFrame(matrix_train_lowrank)\n",
    "combDF_bog_train = pd.concat([df_train.drop(columns=[\"rtweet\",\"tweet\"]), df_bow_train], axis=1)\n",
    "#combDF_bog_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combDF_train = pd.concat([combDF_bog_train, df_tfidf_train], axis=1)\n",
    "#combDF_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262555, 2007)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combDF_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## To store\n",
    "\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# def storeModel(model,filename):\n",
    "#     pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "# SVD = \"SVD_1000_v2\"\n",
    "# storeModel(svd,SVD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "## Preparing testing data\n",
    "#####\n",
    "\n",
    "df_test = pd.DataFrame(sentences_test,columns=[\"rtweet\"])\n",
    "df_test = preprocessingDF(df_test)\n",
    "#df_test = dropFeatures(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalizing test data\n",
    "\n",
    "\n",
    "wdCount_test = df_test['wdCount'].values #returns a numpy array\n",
    "wdCount_test = wdCount_test.reshape(-1, 1)\n",
    "wdCount_scaled_test = wdCount_scaler.transform(wdCount_test)\n",
    "df_test['wdCount'] = pd.DataFrame(wdCount_scaled_test)\n",
    "\n",
    "\n",
    "\n",
    "chCount_test = df_test['chCount'].values #returns a numpy array\n",
    "chCount_test = chCount_test.reshape(-1, 1)\n",
    "\n",
    "chCount_scaled_test = chCount_scaler.transform(chCount_test)\n",
    "df_test['chCount'] = pd.DataFrame(chCount_scaled_test)\n",
    "\n",
    "\n",
    "#df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_bow_test = vectorizer.transform(df_test['tweet'].values)\n",
    "matrix_test_lowrank = svd.transform(X_bow_test)\n",
    "\n",
    "df_bow_test = pd.DataFrame(matrix_test_lowrank)\n",
    "combDF_test = pd.concat([df_test.drop(columns=[\"rtweet\",\"tweet\"]), df_bow_test], axis=1)\n",
    "#combDF_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumLink</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>spCount</th>\n",
       "      <th>wdCount</th>\n",
       "      <th>chCount</th>\n",
       "      <th>isCap</th>\n",
       "      <th>isRT</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.192994</td>\n",
       "      <td>-0.034884</td>\n",
       "      <td>0.099322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>-0.000662</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>0.002191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033413</td>\n",
       "      <td>0.042095</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.517067</td>\n",
       "      <td>0.093948</td>\n",
       "      <td>0.278472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047708</td>\n",
       "      <td>-0.025545</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.046583</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>-0.009843</td>\n",
       "      <td>0.005496</td>\n",
       "      <td>0.022838</td>\n",
       "      <td>0.008453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026253</td>\n",
       "      <td>0.028064</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.558949</td>\n",
       "      <td>-1.392155</td>\n",
       "      <td>-0.359748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022637</td>\n",
       "      <td>-0.017494</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.006798</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>-0.011042</td>\n",
       "      <td>0.011405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040573</td>\n",
       "      <td>0.041160</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737812</td>\n",
       "      <td>0.206194</td>\n",
       "      <td>0.737023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.007097</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>-0.015345</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>-0.010998</td>\n",
       "      <td>-0.006199</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>-0.005848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.853763</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.212842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.002180</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>-0.003505</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054893</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.521867</td>\n",
       "      <td>1.474270</td>\n",
       "      <td>0.239612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009460</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.027583</td>\n",
       "      <td>-0.003945</td>\n",
       "      <td>-0.019125</td>\n",
       "      <td>-0.001194</td>\n",
       "      <td>-0.005433</td>\n",
       "      <td>-0.018909</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.013304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.022919</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368952</td>\n",
       "      <td>0.384042</td>\n",
       "      <td>0.686889</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002683</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.005379</td>\n",
       "      <td>-0.001408</td>\n",
       "      <td>-0.001255</td>\n",
       "      <td>0.000553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107845</td>\n",
       "      <td>0.049721</td>\n",
       "      <td>0.083212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>-0.001838</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>-0.002563</td>\n",
       "      <td>-0.014278</td>\n",
       "      <td>-0.004763</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.001629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038186</td>\n",
       "      <td>0.038354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023509</td>\n",
       "      <td>-0.041828</td>\n",
       "      <td>0.649776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015315</td>\n",
       "      <td>-0.006243</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>-0.010224</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>-0.010409</td>\n",
       "      <td>-0.003530</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>0.032741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.346484</td>\n",
       "      <td>0.120026</td>\n",
       "      <td>0.339969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000388</td>\n",
       "      <td>-0.003235</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>-0.004406</td>\n",
       "      <td>-0.001949</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>-0.003761</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.005738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.002419</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>-0.002744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059666</td>\n",
       "      <td>0.064079</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.233643</td>\n",
       "      <td>2.199004</td>\n",
       "      <td>-0.970976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014251</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>-0.011031</td>\n",
       "      <td>-0.033490</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>-0.015448</td>\n",
       "      <td>-0.016645</td>\n",
       "      <td>-0.001453</td>\n",
       "      <td>-0.019013</td>\n",
       "      <td>-0.013848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040573</td>\n",
       "      <td>0.047240</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532722</td>\n",
       "      <td>0.032265</td>\n",
       "      <td>0.395298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>-0.021236</td>\n",
       "      <td>-0.007161</td>\n",
       "      <td>-0.021411</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>-0.004720</td>\n",
       "      <td>0.020953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767657</td>\n",
       "      <td>0.960853</td>\n",
       "      <td>0.068220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>-0.009982</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.009090</td>\n",
       "      <td>-0.004540</td>\n",
       "      <td>-0.006428</td>\n",
       "      <td>0.033161</td>\n",
       "      <td>-0.010029</td>\n",
       "      <td>0.003093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.045346</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.421620</td>\n",
       "      <td>0.084406</td>\n",
       "      <td>-0.588439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>-0.007697</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>-0.006044</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.012327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.064439</td>\n",
       "      <td>0.064079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.336695</td>\n",
       "      <td>-0.350755</td>\n",
       "      <td>0.263087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>-0.002779</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.004782</td>\n",
       "      <td>-0.001156</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.002498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038186</td>\n",
       "      <td>0.036015</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.812321</td>\n",
       "      <td>2.670264</td>\n",
       "      <td>-2.271969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009974</td>\n",
       "      <td>-0.001764</td>\n",
       "      <td>-0.002147</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>-0.007860</td>\n",
       "      <td>0.017249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.891645</td>\n",
       "      <td>-0.577362</td>\n",
       "      <td>-0.131368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015858</td>\n",
       "      <td>-0.006902</td>\n",
       "      <td>-0.013598</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>-0.003427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>0.028064</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737935</td>\n",
       "      <td>0.252371</td>\n",
       "      <td>0.747892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007630</td>\n",
       "      <td>0.044935</td>\n",
       "      <td>-0.038137</td>\n",
       "      <td>-0.033406</td>\n",
       "      <td>0.014943</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>-0.005326</td>\n",
       "      <td>-0.028421</td>\n",
       "      <td>0.030756</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040456</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>0.048212</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000381</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.002080</td>\n",
       "      <td>-0.000752</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>0.048176</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436345</td>\n",
       "      <td>-0.152682</td>\n",
       "      <td>0.175540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015163</td>\n",
       "      <td>-0.010175</td>\n",
       "      <td>-0.019946</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>-0.005846</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>-0.004901</td>\n",
       "      <td>-0.020173</td>\n",
       "      <td>-0.014451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026253</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.503496</td>\n",
       "      <td>0.107247</td>\n",
       "      <td>-0.608822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007214</td>\n",
       "      <td>-0.001767</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>-0.002169</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>0.003437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>0.055192</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850057</td>\n",
       "      <td>0.799440</td>\n",
       "      <td>-0.325554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>-0.012019</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>-0.005381</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.009585</td>\n",
       "      <td>-0.010848</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>-0.004180</td>\n",
       "      <td>0.016045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.061716</td>\n",
       "      <td>-0.363894</td>\n",
       "      <td>0.270915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>-0.002542</td>\n",
       "      <td>-0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.014032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166932</td>\n",
       "      <td>0.068355</td>\n",
       "      <td>0.069319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>-0.001401</td>\n",
       "      <td>-0.002351</td>\n",
       "      <td>-0.000508</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.002573</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054893</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.239165</td>\n",
       "      <td>1.446411</td>\n",
       "      <td>-0.978229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>-0.011451</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>-0.010756</td>\n",
       "      <td>-0.003141</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>-0.004141</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>-0.003097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.471315</td>\n",
       "      <td>0.680896</td>\n",
       "      <td>-0.516291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>-0.013034</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>-0.009098</td>\n",
       "      <td>-0.008960</td>\n",
       "      <td>0.007999</td>\n",
       "      <td>-0.010834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050119</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.629592</td>\n",
       "      <td>0.139478</td>\n",
       "      <td>-0.475004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>-0.011181</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>-0.002282</td>\n",
       "      <td>-0.009075</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>-0.012193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033863</td>\n",
       "      <td>0.016884</td>\n",
       "      <td>0.029451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>-0.001906</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>0.023849</td>\n",
       "      <td>-0.013239</td>\n",
       "      <td>0.009395</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>-0.015000</td>\n",
       "      <td>0.006620</td>\n",
       "      <td>-0.002018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026253</td>\n",
       "      <td>0.033676</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.912074</td>\n",
       "      <td>-0.753493</td>\n",
       "      <td>-0.143855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002432</td>\n",
       "      <td>-0.001204</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>-0.008420</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>-0.015995</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>-0.020644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>0.045837</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931103</td>\n",
       "      <td>0.201935</td>\n",
       "      <td>1.107221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008949</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>-0.020808</td>\n",
       "      <td>0.003339</td>\n",
       "      <td>0.034771</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.001258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992639</td>\n",
       "      <td>0.830284</td>\n",
       "      <td>-0.064131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001985</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>-0.000548</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.001136</td>\n",
       "      <td>0.001933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026253</td>\n",
       "      <td>0.024790</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.079029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>-0.001687</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>-0.007405</td>\n",
       "      <td>-0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.033413</td>\n",
       "      <td>0.035547</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.718711</td>\n",
       "      <td>0.501569</td>\n",
       "      <td>1.186765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>-0.005322</td>\n",
       "      <td>-0.002094</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>-0.005093</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>-0.001271</td>\n",
       "      <td>-0.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65613</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252711</td>\n",
       "      <td>0.030592</td>\n",
       "      <td>0.132219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>-0.009909</td>\n",
       "      <td>-0.002330</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>-0.003412</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65614</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>0.036483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.218834</td>\n",
       "      <td>0.022890</td>\n",
       "      <td>0.141285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037777</td>\n",
       "      <td>0.033117</td>\n",
       "      <td>-0.009193</td>\n",
       "      <td>0.033164</td>\n",
       "      <td>0.029193</td>\n",
       "      <td>0.015857</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>-0.022178</td>\n",
       "      <td>-0.049504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65615</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.028064</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550002</td>\n",
       "      <td>0.037317</td>\n",
       "      <td>0.409025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005281</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>-0.017998</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>-0.013709</td>\n",
       "      <td>0.025651</td>\n",
       "      <td>-0.049975</td>\n",
       "      <td>-0.014354</td>\n",
       "      <td>-0.003077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65616</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.052853</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523487</td>\n",
       "      <td>0.027097</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002172</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.006173</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000973</td>\n",
       "      <td>-0.001481</td>\n",
       "      <td>-0.002656</td>\n",
       "      <td>-0.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65617</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009547</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225569</td>\n",
       "      <td>0.164923</td>\n",
       "      <td>0.360823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>-0.002178</td>\n",
       "      <td>-0.002888</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>0.001059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65618</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033413</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532531</td>\n",
       "      <td>-0.035899</td>\n",
       "      <td>0.375378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.016232</td>\n",
       "      <td>-0.000895</td>\n",
       "      <td>-0.007033</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.010991</td>\n",
       "      <td>-0.014856</td>\n",
       "      <td>-0.016210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65619</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.047733</td>\n",
       "      <td>0.053321</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.252051</td>\n",
       "      <td>0.833945</td>\n",
       "      <td>0.111269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003149</td>\n",
       "      <td>-0.009119</td>\n",
       "      <td>-0.019476</td>\n",
       "      <td>-0.011194</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>0.018649</td>\n",
       "      <td>-0.011113</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>-0.006560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65620</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047733</td>\n",
       "      <td>0.051918</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.338543</td>\n",
       "      <td>-0.596294</td>\n",
       "      <td>0.178103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>-0.000427</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>-0.002823</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>-0.002288</td>\n",
       "      <td>-0.006911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65621</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>0.018709</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.642076</td>\n",
       "      <td>-1.091654</td>\n",
       "      <td>0.084345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005873</td>\n",
       "      <td>-0.007811</td>\n",
       "      <td>-0.008210</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.008877</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.004088</td>\n",
       "      <td>0.017603</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>-0.006437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65622</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251238</td>\n",
       "      <td>0.271903</td>\n",
       "      <td>0.572189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>-0.000243</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.000739</td>\n",
       "      <td>-0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009547</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067437</td>\n",
       "      <td>0.021255</td>\n",
       "      <td>0.040536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>-0.004869</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>-0.003322</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>-0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65624</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.896027</td>\n",
       "      <td>-0.536386</td>\n",
       "      <td>-0.213900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.002830</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65625</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.396372</td>\n",
       "      <td>-1.017802</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>-0.022605</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.023210</td>\n",
       "      <td>0.009502</td>\n",
       "      <td>-0.012825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65626</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.026193</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857550</td>\n",
       "      <td>-0.768920</td>\n",
       "      <td>-0.170692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>-0.001451</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>-0.000963</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>0.001165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65627</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>0.049579</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658654</td>\n",
       "      <td>0.033813</td>\n",
       "      <td>0.456121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008686</td>\n",
       "      <td>-0.021538</td>\n",
       "      <td>0.013635</td>\n",
       "      <td>-0.014572</td>\n",
       "      <td>-0.006573</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>-0.000965</td>\n",
       "      <td>-0.007737</td>\n",
       "      <td>0.035317</td>\n",
       "      <td>0.049074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65628</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021480</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.792240</td>\n",
       "      <td>1.003330</td>\n",
       "      <td>0.062780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020235</td>\n",
       "      <td>-0.023922</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>-0.030260</td>\n",
       "      <td>0.017076</td>\n",
       "      <td>-0.016857</td>\n",
       "      <td>0.034521</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.048765</td>\n",
       "      <td>0.005034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65629</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.020627</td>\n",
       "      <td>1.070945</td>\n",
       "      <td>0.326162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.003482</td>\n",
       "      <td>-0.009020</td>\n",
       "      <td>-0.004760</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.008258</td>\n",
       "      <td>0.004191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65630</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.575395</td>\n",
       "      <td>0.045446</td>\n",
       "      <td>0.444520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>-0.014378</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>-0.002776</td>\n",
       "      <td>-0.010713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65631</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026253</td>\n",
       "      <td>0.035080</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.948739</td>\n",
       "      <td>-0.650095</td>\n",
       "      <td>-0.059704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009522</td>\n",
       "      <td>-0.021183</td>\n",
       "      <td>0.012731</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.028693</td>\n",
       "      <td>0.015334</td>\n",
       "      <td>-0.012865</td>\n",
       "      <td>0.024218</td>\n",
       "      <td>0.007648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65632</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163876</td>\n",
       "      <td>0.084589</td>\n",
       "      <td>0.097887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>-0.010193</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>-0.002737</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-0.001958</td>\n",
       "      <td>-0.002658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65633</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.015435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082726</td>\n",
       "      <td>0.039483</td>\n",
       "      <td>0.085701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004669</td>\n",
       "      <td>-0.002337</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>-0.000718</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>-0.007714</td>\n",
       "      <td>-0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65634</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.047733</td>\n",
       "      <td>0.052385</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.732447</td>\n",
       "      <td>0.247872</td>\n",
       "      <td>-0.190286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003602</td>\n",
       "      <td>-0.009900</td>\n",
       "      <td>-0.014869</td>\n",
       "      <td>-0.009604</td>\n",
       "      <td>0.010983</td>\n",
       "      <td>-0.003468</td>\n",
       "      <td>0.008147</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.006685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65635</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086682</td>\n",
       "      <td>0.033860</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>-0.008047</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.011588</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.002727</td>\n",
       "      <td>-0.010308</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65636</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>0.018709</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.237291</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.093827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>-0.001422</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>0.001937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65637</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.025725</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066361</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>0.035063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>-0.010407</td>\n",
       "      <td>-0.007023</td>\n",
       "      <td>-0.025238</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>0.012391</td>\n",
       "      <td>-0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65638</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.032273</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.678235</td>\n",
       "      <td>0.613878</td>\n",
       "      <td>-0.468142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016853</td>\n",
       "      <td>-0.004891</td>\n",
       "      <td>-0.008753</td>\n",
       "      <td>-0.048425</td>\n",
       "      <td>-0.036369</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>-0.015514</td>\n",
       "      <td>-0.032328</td>\n",
       "      <td>-0.005933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65639 rows × 2007 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NumLink  hashtags  spCount   wdCount   chCount  isCap  isRT         0  \\\n",
       "0            0         0        1  0.004773  0.007016      0     0  0.192994   \n",
       "1            1         0        0  0.033413  0.042095      5     0  0.517067   \n",
       "2            1         0        2  0.026253  0.028064      7     0  1.558949   \n",
       "3            1         0        3  0.040573  0.041160      9     0  0.737812   \n",
       "4            0         0        0  0.035800  0.040225      3     0  1.853763   \n",
       "5            0         0        3  0.054893  0.051450      7     0  2.521867   \n",
       "6            0         0        1  0.019093  0.022919      8     0  0.368952   \n",
       "7            0         0        1  0.014320  0.020112      0     0  0.107845   \n",
       "8            0         0        1  0.038186  0.038354      0     0  1.023509   \n",
       "9            0         0        3  0.028640  0.032741      0     0  0.346484   \n",
       "10           0         0        2  0.002387  0.003742      0     0  0.005577   \n",
       "11           0         0        0  0.059666  0.064079     10     0  2.233643   \n",
       "12           0         0        3  0.040573  0.047240      6     0  0.532722   \n",
       "13           0         0        0  0.023866  0.023854      0     0  0.767657   \n",
       "14           0         0        4  0.045346  0.051450      9     0  1.421620   \n",
       "15           0         0        2  0.064439  0.064079      0     0  1.336695   \n",
       "16           0         0        0  0.038186  0.036015      4     0  1.812321   \n",
       "17           0         0        0  0.035800  0.044434      4     0  0.891645   \n",
       "18           1         0        1  0.028640  0.028064      2     0  0.737935   \n",
       "19           0         0        1  0.002387  0.005145      1     0  0.040456   \n",
       "20           0         0        0  0.028640  0.048176      7     0  0.436345   \n",
       "21           0         0        2  0.026253  0.025257      1     0  1.503496   \n",
       "22           0         0        2  0.042959  0.055192      8     0  0.850057   \n",
       "23           0         0        2  0.019093  0.020580      2     0  1.061716   \n",
       "24           0         0        0  0.014320  0.014032      0     0  0.166932   \n",
       "25           0         0        3  0.054893  0.055659      7     0  1.239165   \n",
       "26           0         0        0  0.014320  0.028531      1     0  0.471315   \n",
       "27           0         0        1  0.050119  0.055659      3     0  1.629592   \n",
       "28           0         0        3  0.004773  0.007484      0     0  0.033863   \n",
       "29           0         1        2  0.026253  0.033676      7     0  0.912074   \n",
       "...        ...       ...      ...       ...       ...    ...   ...       ...   \n",
       "65609        0         0        1  0.042959  0.045837      1     0  0.931103   \n",
       "65610        0         0        1  0.028640  0.026660      2     1  0.992639   \n",
       "65611        0         0        1  0.026253  0.024790      3     0  0.180521   \n",
       "65612        0         0        6  0.033413  0.035547      8     0  0.718711   \n",
       "65613        0         0        0  0.023866  0.025257      3     0  0.252711   \n",
       "65614        0         0        0  0.016706  0.036483      0     0  0.218834   \n",
       "65615        0         0        0  0.019093  0.028064      1     0  0.550002   \n",
       "65616        0         1        2  0.035800  0.052853      3     0  0.523487   \n",
       "65617        0         0        0  0.009547  0.010758      2     0  0.225569   \n",
       "65618        0         0        1  0.033413  0.037886      1     0  0.532531   \n",
       "65619        0         0        2  0.047733  0.053321      2     0  1.252051   \n",
       "65620        0         0        1  0.047733  0.051918      4     0  1.338543   \n",
       "65621        0         0        0  0.016706  0.018709      1     0  1.642076   \n",
       "65622        0         0        2  0.002387  0.003742      0     0  0.251238   \n",
       "65623        0         0        0  0.009547  0.012629      1     0  0.067437   \n",
       "65624        0         0        1  0.023866  0.023386      3     0  0.896027   \n",
       "65625        0         0        3  0.042959  0.055659      7     0  1.396372   \n",
       "65626        1         0        1  0.019093  0.026193      6     0  0.857550   \n",
       "65627        0         1        3  0.042959  0.049579      4     0  0.658654   \n",
       "65628        0         0        0  0.021480  0.026660      0     0  0.792240   \n",
       "65629        0         0        1  0.023866  0.025257      2     0  1.020627   \n",
       "65630        0         0        0  0.019093  0.020580      1     0  0.575395   \n",
       "65631        0         0        0  0.026253  0.035080      3     0  0.948739   \n",
       "65632        0         0        1  0.014320  0.020580      3     0  0.163876   \n",
       "65633        0         0        0  0.014320  0.015435      0     0  0.082726   \n",
       "65634        0         0        2  0.047733  0.052385      1     0  1.732447   \n",
       "65635        0         0        1  0.023866  0.026660      6     0  0.086682   \n",
       "65636        0         0        0  0.011933  0.018709      4     0  0.237291   \n",
       "65637        0         0        2  0.023866  0.025725      4     0  0.066361   \n",
       "65638        0         1        2  0.023866  0.032273      9     0  0.678235   \n",
       "\n",
       "              1         2  ...       990       991       992       993  \\\n",
       "0     -0.034884  0.099322  ...  0.002936  0.000831  0.001480  0.001998   \n",
       "1      0.093948  0.278472  ... -0.047708 -0.025545  0.006834  0.046583   \n",
       "2     -1.392155 -0.359748  ...  0.022637 -0.017494  0.010043  0.014480   \n",
       "3      0.206194  0.737023  ...  0.002685  0.007097 -0.002027 -0.015345   \n",
       "4      0.415569  0.212842  ...  0.004941 -0.004954  0.002754  0.000551   \n",
       "5      1.474270  0.239612  ... -0.009460  0.007135  0.027583 -0.003945   \n",
       "6      0.384042  0.686889  ... -0.002683  0.000581 -0.000716 -0.002572   \n",
       "7      0.049721  0.083212  ...  0.001302 -0.001838 -0.000244 -0.002563   \n",
       "8     -0.041828  0.649776  ... -0.015315 -0.006243  0.003596  0.008603   \n",
       "9      0.120026  0.339969  ... -0.000388 -0.003235  0.000079 -0.004406   \n",
       "10     0.000007  0.002781  ... -0.000327 -0.002419  0.006349  0.005531   \n",
       "11     2.199004 -0.970976  ... -0.014251  0.016279 -0.011031 -0.033490   \n",
       "12     0.032265  0.395298  ...  0.000403 -0.021236 -0.007161 -0.021411   \n",
       "13     0.960853  0.068220  ...  0.006340  0.002431 -0.009982 -0.009349   \n",
       "14     0.084406 -0.588439  ...  0.000121  0.003133  0.002251 -0.007697   \n",
       "15    -0.350755  0.263087  ...  0.004700  0.002581 -0.002834 -0.002779   \n",
       "16     2.670264 -2.271969  ... -0.009974 -0.001764 -0.002147 -0.000710   \n",
       "17    -0.577362 -0.131368  ... -0.015858 -0.006902 -0.013598  0.010102   \n",
       "18     0.252371  0.747892  ... -0.007630  0.044935 -0.038137 -0.033406   \n",
       "19     0.013115  0.048212  ... -0.000381  0.000057  0.000972 -0.001193   \n",
       "20    -0.152682  0.175540  ...  0.015163 -0.010175 -0.019946  0.014110   \n",
       "21     0.107247 -0.608822  ... -0.007214 -0.001767  0.000757  0.004308   \n",
       "22     0.799440 -0.325554  ...  0.013624 -0.012019  0.014602 -0.005381   \n",
       "23    -0.363894  0.270915  ... -0.002415  0.002992  0.003075 -0.000182   \n",
       "24     0.068355  0.069319  ...  0.000321 -0.001833 -0.001401 -0.002351   \n",
       "25     1.446411 -0.978229  ...  0.004067  0.001447 -0.011451  0.004898   \n",
       "26     0.680896 -0.516291  ...  0.003515  0.002570 -0.013034 -0.009922   \n",
       "27     0.139478 -0.475004  ...  0.006129 -0.011181  0.003812  0.004093   \n",
       "28     0.016884  0.029451  ...  0.006943 -0.001906 -0.001637  0.023849   \n",
       "29    -0.753493 -0.143855  ... -0.002432 -0.001204  0.004991 -0.008420   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "65609  0.201935  1.107221  ... -0.008949  0.000499  0.003536 -0.020808   \n",
       "65610  0.830284 -0.064131  ... -0.001985 -0.003297  0.001719  0.001558   \n",
       "65611  0.000406  0.079029  ...  0.001209 -0.001687  0.012814 -0.000463   \n",
       "65612  0.501569  1.186765  ...  0.000591 -0.001815 -0.005322 -0.002094   \n",
       "65613  0.030592  0.132219  ...  0.002906  0.008082 -0.009909 -0.002330   \n",
       "65614  0.022890  0.141285  ...  0.037777  0.033117 -0.009193  0.033164   \n",
       "65615  0.037317  0.409025  ... -0.005281 -0.009766 -0.008751 -0.017998   \n",
       "65616  0.027097  0.394926  ... -0.002172 -0.000898  0.004030 -0.000578   \n",
       "65617  0.164923  0.360823  ... -0.000470  0.001035  0.001039  0.001130   \n",
       "65618 -0.035899  0.375378  ...  0.001114  0.011561  0.016232 -0.000895   \n",
       "65619  0.833945  0.111269  ... -0.003149 -0.009119 -0.019476 -0.011194   \n",
       "65620 -0.596294  0.178103  ...  0.004469  0.003078 -0.000678  0.003676   \n",
       "65621 -1.091654  0.084345  ... -0.005873 -0.007811 -0.008210  0.004594   \n",
       "65622  0.271903  0.572189  ... -0.001028 -0.000882 -0.000844  0.001963   \n",
       "65623  0.021255  0.040536  ...  0.003026 -0.004869  0.002308  0.001143   \n",
       "65624 -0.536386 -0.213900  ... -0.002514  0.000207  0.001325  0.000218   \n",
       "65625 -1.017802  0.007089  ...  0.007706 -0.022605 -0.024923  0.030562   \n",
       "65626 -0.768920 -0.170692  ...  0.003866  0.002774  0.001307  0.001366   \n",
       "65627  0.033813  0.456121  ... -0.008686 -0.021538  0.013635 -0.014572   \n",
       "65628  1.003330  0.062780  ...  0.020235 -0.023922 -0.002627 -0.030260   \n",
       "65629  1.070945  0.326162  ...  0.002567  0.000235 -0.003482 -0.009020   \n",
       "65630  0.045446  0.444520  ...  0.006017  0.000403  0.002219  0.012824   \n",
       "65631 -0.650095 -0.059704  ... -0.009522 -0.021183  0.012731  0.001206   \n",
       "65632  0.084589  0.097887  ...  0.001116  0.006492 -0.010193  0.001242   \n",
       "65633  0.039483  0.085701  ... -0.004669 -0.002337  0.000741 -0.000542   \n",
       "65634  0.247872 -0.190286  ... -0.003602 -0.009900 -0.014869 -0.009604   \n",
       "65635  0.033860  0.054902  ...  0.005331 -0.008047  0.001660  0.004797   \n",
       "65636  0.007113  0.093827  ...  0.001248  0.001235 -0.000937  0.000087   \n",
       "65637  0.030794  0.035063  ...  0.001866 -0.010407 -0.007023 -0.025238   \n",
       "65638  0.613878 -0.468142  ...  0.016853 -0.004891 -0.008753 -0.048425   \n",
       "\n",
       "            994       995       996       997       998       999  \n",
       "0     -0.000662  0.001764  0.000816 -0.000357 -0.000204  0.002191  \n",
       "1     -0.000862  0.030476 -0.009843  0.005496  0.022838  0.008453  \n",
       "2      0.006798 -0.009007  0.000924 -0.001123 -0.011042  0.011405  \n",
       "3      0.002407 -0.010998 -0.006199  0.000233  0.000223 -0.005848  \n",
       "4      0.002863 -0.000790 -0.002180  0.000387 -0.003505  0.000541  \n",
       "5     -0.019125 -0.001194 -0.005433 -0.018909  0.006435  0.013304  \n",
       "6      0.000543 -0.002665 -0.005379 -0.001408 -0.001255  0.000553  \n",
       "7     -0.014278 -0.004763  0.008917  0.000227  0.016105  0.001629  \n",
       "8     -0.010224  0.015707 -0.010409 -0.003530  0.005275  0.001253  \n",
       "9     -0.001949  0.007234 -0.003761 -0.000397  0.005012  0.005738  \n",
       "10     0.007871  0.006136 -0.000079  0.006754 -0.000413 -0.002744  \n",
       "11     0.009881 -0.015448 -0.016645 -0.001453 -0.019013 -0.013848  \n",
       "12     0.002913  0.011132  0.009299  0.004961 -0.004720  0.020953  \n",
       "13    -0.009090 -0.004540 -0.006428  0.033161 -0.010029  0.003093  \n",
       "14     0.006665 -0.006044  0.000320  0.004838  0.006273  0.012327  \n",
       "15    -0.003006 -0.004782 -0.001156  0.004101 -0.000337 -0.002498  \n",
       "16     0.007597  0.003287  0.008082  0.004659 -0.007860  0.017249  \n",
       "17     0.002896 -0.009345  0.014702 -0.007245 -0.003985 -0.003427  \n",
       "18     0.014943  0.014357 -0.005326 -0.028421  0.030756  0.001609  \n",
       "19     0.000161 -0.000270 -0.002080 -0.000752  0.000593  0.000324  \n",
       "20     0.008272 -0.005846  0.008762 -0.004901 -0.020173 -0.014451  \n",
       "21    -0.000802  0.002821 -0.001410 -0.002169 -0.000994  0.003437  \n",
       "22    -0.006700 -0.009585 -0.010848 -0.005378 -0.004180  0.016045  \n",
       "23     0.000070  0.000873 -0.003547 -0.003927 -0.002542 -0.001397  \n",
       "24    -0.000508  0.002369 -0.000733 -0.002573 -0.001015  0.000192  \n",
       "25    -0.010756 -0.003141  0.009749 -0.004141  0.005865 -0.003097  \n",
       "26     0.000088  0.005350 -0.009098 -0.008960  0.007999 -0.010834  \n",
       "27     0.008271 -0.002282 -0.009075  0.006462 -0.009922 -0.012193  \n",
       "28    -0.013239  0.009395  0.001698 -0.015000  0.006620 -0.002018  \n",
       "29     0.002615  0.021564 -0.015995  0.013724  0.003540 -0.020644  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "65609  0.003339  0.034771  0.013144  0.020057  0.001678  0.001258  \n",
       "65610  0.002049 -0.000548 -0.000833 -0.000271 -0.001136  0.001933  \n",
       "65611  0.001967  0.006492 -0.001080  0.002642 -0.007405 -0.000180  \n",
       "65612 -0.000260  0.000813 -0.005093  0.000932 -0.001271 -0.000811  \n",
       "65613 -0.007086  0.005495  0.001882 -0.003412 -0.001596 -0.004584  \n",
       "65614  0.029193  0.015857  0.006784  0.010884 -0.022178 -0.049504  \n",
       "65615  0.007087 -0.013709  0.025651 -0.049975 -0.014354 -0.003077  \n",
       "65616 -0.006173 -0.000068 -0.000973 -0.001481 -0.002656 -0.000578  \n",
       "65617  0.002385 -0.000506 -0.002178 -0.002888 -0.001953  0.001059  \n",
       "65618 -0.007033  0.011160  0.003552  0.010991 -0.014856 -0.016210  \n",
       "65619 -0.002227  0.018649 -0.011113 -0.005139  0.006970 -0.006560  \n",
       "65620 -0.000427  0.009823 -0.002823  0.000243 -0.002288 -0.006911  \n",
       "65621  0.008877  0.003560  0.004088  0.017603  0.007104 -0.006437  \n",
       "65622 -0.000875  0.000897 -0.000243  0.000062 -0.000739 -0.000167  \n",
       "65623  0.004594 -0.003322  0.002608  0.000397 -0.000529 -0.000106  \n",
       "65624  0.002382 -0.000268 -0.002830  0.000434  0.001500  0.000504  \n",
       "65625  0.001434  0.000232  0.005890  0.023210  0.009502 -0.012825  \n",
       "65626  0.002348 -0.001451 -0.001877 -0.000963 -0.000652  0.001165  \n",
       "65627 -0.006573  0.003373 -0.000965 -0.007737  0.035317  0.049074  \n",
       "65628  0.017076 -0.016857  0.034521  0.009120  0.048765  0.005034  \n",
       "65629 -0.004760 -0.006720  0.001647  0.002258 -0.008258  0.004191  \n",
       "65630 -0.014378  0.010720  0.011989  0.006163 -0.002776 -0.010713  \n",
       "65631  0.001302  0.028693  0.015334 -0.012865  0.024218  0.007648  \n",
       "65632 -0.000269 -0.000260 -0.002737  0.001119 -0.001958 -0.002658  \n",
       "65633  0.003168  0.001904 -0.000718  0.001441 -0.007714 -0.000643  \n",
       "65634  0.010983 -0.003468  0.008147 -0.005747  0.001541  0.006685  \n",
       "65635  0.011588  0.005408  0.000732 -0.002727 -0.010308  0.006700  \n",
       "65636  0.000088  0.000206  0.001060 -0.001422 -0.000413  0.001937  \n",
       "65637  0.010735  0.008639  0.004028 -0.002215  0.012391 -0.002800  \n",
       "65638 -0.036369  0.009105  0.010704 -0.015514 -0.032328 -0.005933  \n",
       "\n",
       "[65639 rows x 2007 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_test = tfidf_vecs.transform(df_test['tweet'].values)\n",
    "\n",
    "\n",
    "matrix_tfidf_test_lowrank = svd_tfidf.transform(X_tfidf_test)\n",
    "\n",
    "df_tfidf_test = pd.DataFrame(matrix_tfidf_test_lowrank)\n",
    "combDF_test_total = pd.concat([combDF_test, df_tfidf_test], axis=1)\n",
    "combDF_test_total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65639, 2007)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combDF_test_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = combDF_train.values\n",
    "X_test = combDF_test_total.values\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "# model.add(Dense(1000, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dense(100, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dense(250, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\n",
    "model.add(Dense(1000, input_dim=input_dim, activation='relu')) # input dimension = dimension of festure vector\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(9293, activation='softmax')) # output layer = no. of classes\n",
    "\n",
    "#sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 262555 samples, validate on 65639 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a91f5187734f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_data=(X_test,y_test))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \"\"\"\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_add_should_use_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m   return tf_decorator.make_decorator(\n\u001b[1;32m    120\u001b[0m       \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'should_use_result'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\u001b[0m in \u001b[0;36m_add_should_use_warning\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     setattr(TFShouldUseWarningWrapper, name,\n\u001b[0;32m---> 85\u001b[0;31m             functools.wraps(method)(override_method(method)))\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFShouldUseWarningWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/functools.py\u001b[0m in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     57\u001b[0m        \u001b[0mfunction\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRAPPER_UPDATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \"\"\"\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "epochs = 20\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 262555 samples, validate on 65639 samples\n",
      "Epoch 1/30\n",
      "262555/262555 [==============================] - 134s 511us/step - loss: 7.3421 - acc: 0.0352 - val_loss: 7.7090 - val_acc: 0.0346\n",
      "Epoch 2/30\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 7.2533 - acc: 0.0394 - val_loss: 7.6215 - val_acc: 0.0384\n",
      "Epoch 3/30\n",
      "262555/262555 [==============================] - 127s 484us/step - loss: 7.1639 - acc: 0.0425 - val_loss: 7.5385 - val_acc: 0.0461\n",
      "Epoch 4/30\n",
      "262555/262555 [==============================] - 129s 490us/step - loss: 7.0695 - acc: 0.0464 - val_loss: 7.6012 - val_acc: 0.0446\n",
      "Epoch 5/30\n",
      "262555/262555 [==============================] - 118s 449us/step - loss: 6.9885 - acc: 0.0498 - val_loss: 7.6795 - val_acc: 0.0427\n",
      "Epoch 6/30\n",
      "262555/262555 [==============================] - 116s 441us/step - loss: 6.9174 - acc: 0.0526 - val_loss: 7.6781 - val_acc: 0.0447\n",
      "Epoch 7/30\n",
      "262555/262555 [==============================] - 120s 458us/step - loss: 6.8561 - acc: 0.0548 - val_loss: 7.5381 - val_acc: 0.0515s - loss: 6.8550 - acc: 0.\n",
      "Epoch 8/30\n",
      "262555/262555 [==============================] - 118s 448us/step - loss: 6.7841 - acc: 0.0569 - val_loss: 7.6080 - val_acc: 0.0516\n",
      "Epoch 9/30\n",
      "262555/262555 [==============================] - 119s 454us/step - loss: 6.7236 - acc: 0.0602 - val_loss: 7.5962 - val_acc: 0.0552\n",
      "Epoch 10/30\n",
      "262555/262555 [==============================] - 135s 516us/step - loss: 6.6682 - acc: 0.0627 - val_loss: 7.5440 - val_acc: 0.0573\n",
      "Epoch 11/30\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 6.6178 - acc: 0.0639 - val_loss: 7.5345 - val_acc: 0.0593\n",
      "Epoch 12/30\n",
      "262555/262555 [==============================] - 128s 487us/step - loss: 6.5619 - acc: 0.0658 - val_loss: 7.7257 - val_acc: 0.0530\n",
      "Epoch 13/30\n",
      "262555/262555 [==============================] - 118s 450us/step - loss: 6.4986 - acc: 0.0688 - val_loss: 7.7040 - val_acc: 0.0534\n",
      "Epoch 14/30\n",
      "262555/262555 [==============================] - 119s 454us/step - loss: 6.4609 - acc: 0.0700 - val_loss: 7.6181 - val_acc: 0.0603\n",
      "Epoch 15/30\n",
      "262555/262555 [==============================] - 119s 455us/step - loss: 6.4142 - acc: 0.0722 - val_loss: 7.5926 - val_acc: 0.0640\n",
      "Epoch 16/30\n",
      "262555/262555 [==============================] - 130s 497us/step - loss: 6.3658 - acc: 0.0742 - val_loss: 7.6886 - val_acc: 0.0608\n",
      "Epoch 17/30\n",
      "262555/262555 [==============================] - 113s 430us/step - loss: 6.3248 - acc: 0.0756 - val_loss: 7.7090 - val_acc: 0.0610\n",
      "Epoch 18/30\n",
      "262555/262555 [==============================] - 117s 444us/step - loss: 6.2762 - acc: 0.0772 - val_loss: 7.7417 - val_acc: 0.0617\n",
      "Epoch 19/30\n",
      "262555/262555 [==============================] - 114s 432us/step - loss: 6.2324 - acc: 0.0801 - val_loss: 7.8175 - val_acc: 0.0587\n",
      "Epoch 20/30\n",
      "262555/262555 [==============================] - 111s 423us/step - loss: 6.2071 - acc: 0.0799 - val_loss: 7.7319 - val_acc: 0.0659\n",
      "Epoch 21/30\n",
      "262555/262555 [==============================] - 115s 437us/step - loss: 6.1650 - acc: 0.0820 - val_loss: 7.7651 - val_acc: 0.0634\n",
      "Epoch 22/30\n",
      "262555/262555 [==============================] - 132s 503us/step - loss: 6.1222 - acc: 0.0838 - val_loss: 7.8270 - val_acc: 0.0638\n",
      "Epoch 23/30\n",
      "262555/262555 [==============================] - 121s 460us/step - loss: 6.0813 - acc: 0.0857 - val_loss: 7.8449 - val_acc: 0.0633\n",
      "Epoch 24/30\n",
      "262555/262555 [==============================] - 121s 460us/step - loss: 6.0634 - acc: 0.0873 - val_loss: 7.9909 - val_acc: 0.0590\n",
      "Epoch 25/30\n",
      "262555/262555 [==============================] - 130s 495us/step - loss: 6.0274 - acc: 0.0889 - val_loss: 8.0620 - val_acc: 0.061076 - acc: 0.088\n",
      "Epoch 26/30\n",
      "262555/262555 [==============================] - 128s 489us/step - loss: 5.9916 - acc: 0.0900 - val_loss: 7.8759 - val_acc: 0.0639  - ETA: 3s - loss: 5\n",
      "Epoch 27/30\n",
      "262555/262555 [==============================] - 121s 459us/step - loss: 5.9706 - acc: 0.0907 - val_loss: 7.9694 - val_acc: 0.0640\n",
      "Epoch 28/30\n",
      "262555/262555 [==============================] - 129s 492us/step - loss: 5.9281 - acc: 0.0936 - val_loss: 8.0353 - val_acc: 0.0616\n",
      "Epoch 29/30\n",
      "262555/262555 [==============================] - 131s 499us/step - loss: 5.9108 - acc: 0.0939 - val_loss: 7.9745 - val_acc: 0.0630\n",
      "Epoch 30/30\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 5.8915 - acc: 0.0952 - val_loss: 8.0127 - val_acc: 0.0671\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "epochs = 30\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 262555 samples, validate on 65639 samples\n",
      "Epoch 1/300\n",
      "262555/262555 [==============================] - 134s 510us/step - loss: 5.8561 - acc: 0.0973 - val_loss: 8.0946 - val_acc: 0.0657\n",
      "Epoch 2/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 5.8329 - acc: 0.0988 - val_loss: 8.0644 - val_acc: 0.0663\n",
      "Epoch 3/300\n",
      "262555/262555 [==============================] - 125s 475us/step - loss: 5.8043 - acc: 0.0995 - val_loss: 8.1641 - val_acc: 0.0621\n",
      "Epoch 4/300\n",
      "262555/262555 [==============================] - 119s 454us/step - loss: 5.7882 - acc: 0.1004 - val_loss: 8.1390 - val_acc: 0.0667\n",
      "Epoch 5/300\n",
      "262555/262555 [==============================] - 122s 466us/step - loss: 5.7546 - acc: 0.1034 - val_loss: 8.1014 - val_acc: 0.0661\n",
      "Epoch 6/300\n",
      "262555/262555 [==============================] - 120s 456us/step - loss: 5.7401 - acc: 0.1035 - val_loss: 8.2135 - val_acc: 0.0587\n",
      "Epoch 7/300\n",
      "262555/262555 [==============================] - 124s 474us/step - loss: 5.7109 - acc: 0.1054 - val_loss: 8.0797 - val_acc: 0.0708\n",
      "Epoch 8/300\n",
      "262555/262555 [==============================] - 121s 463us/step - loss: 5.6889 - acc: 0.1063 - val_loss: 8.1488 - val_acc: 0.0666\n",
      "Epoch 9/300\n",
      "262555/262555 [==============================] - 120s 459us/step - loss: 5.6701 - acc: 0.1079 - val_loss: 8.2537 - val_acc: 0.065676 -\n",
      "Epoch 10/300\n",
      "262555/262555 [==============================] - 124s 470us/step - loss: 5.6582 - acc: 0.1083 - val_loss: 8.1011 - val_acc: 0.0704\n",
      "Epoch 11/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 5.6380 - acc: 0.1100 - val_loss: 8.2713 - val_acc: 0.0672\n",
      "Epoch 12/300\n",
      "262555/262555 [==============================] - 127s 483us/step - loss: 5.6143 - acc: 0.1120 - val_loss: 8.1942 - val_acc: 0.0692\n",
      "Epoch 13/300\n",
      "262555/262555 [==============================] - 121s 461us/step - loss: 5.5975 - acc: 0.1130 - val_loss: 8.2314 - val_acc: 0.0692 a\n",
      "Epoch 14/300\n",
      "262555/262555 [==============================] - 118s 450us/step - loss: 5.5842 - acc: 0.1135 - val_loss: 8.2074 - val_acc: 0.0692\n",
      "Epoch 15/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 5.5736 - acc: 0.1142 - val_loss: 8.2698 - val_acc: 0.0667\n",
      "Epoch 16/300\n",
      "262555/262555 [==============================] - 121s 459us/step - loss: 5.5395 - acc: 0.1164 - val_loss: 8.4034 - val_acc: 0.0630\n",
      "Epoch 17/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 5.5223 - acc: 0.1178 - val_loss: 8.2894 - val_acc: 0.0645\n",
      "Epoch 18/300\n",
      "262555/262555 [==============================] - 119s 453us/step - loss: 5.4960 - acc: 0.1198 - val_loss: 8.3864 - val_acc: 0.0661\n",
      "Epoch 19/300\n",
      "262555/262555 [==============================] - 119s 454us/step - loss: 5.4951 - acc: 0.1195 - val_loss: 8.3232 - val_acc: 0.0680\n",
      "Epoch 20/300\n",
      "262555/262555 [==============================] - 128s 489us/step - loss: 5.4809 - acc: 0.1205 - val_loss: 8.3937 - val_acc: 0.0660\n",
      "Epoch 21/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 5.4691 - acc: 0.1213 - val_loss: 8.4490 - val_acc: 0.0632\n",
      "Epoch 22/300\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 5.4555 - acc: 0.1228 - val_loss: 8.3587 - val_acc: 0.0740\n",
      "Epoch 23/300\n",
      "262555/262555 [==============================] - 124s 472us/step - loss: 5.4261 - acc: 0.1247 - val_loss: 8.6390 - val_acc: 0.0610\n",
      "Epoch 24/300\n",
      "262555/262555 [==============================] - 120s 458us/step - loss: 5.4406 - acc: 0.1234 - val_loss: 8.5880 - val_acc: 0.0603\n",
      "Epoch 25/300\n",
      "262555/262555 [==============================] - 126s 479us/step - loss: 5.4412 - acc: 0.1234 - val_loss: 8.4096 - val_acc: 0.0674\n",
      "Epoch 26/300\n",
      "262555/262555 [==============================] - 127s 483us/step - loss: 5.4051 - acc: 0.1256 - val_loss: 8.4592 - val_acc: 0.0647\n",
      "Epoch 27/300\n",
      "262555/262555 [==============================] - 127s 484us/step - loss: 5.3887 - acc: 0.1271 - val_loss: 8.5170 - val_acc: 0.0656\n",
      "Epoch 28/300\n",
      "262555/262555 [==============================] - 128s 486us/step - loss: 5.3702 - acc: 0.1286 - val_loss: 8.3819 - val_acc: 0.0669\n",
      "Epoch 29/300\n",
      "262555/262555 [==============================] - 137s 523us/step - loss: 5.3691 - acc: 0.1283 - val_loss: 8.4704 - val_acc: 0.0681\n",
      "Epoch 30/300\n",
      "262555/262555 [==============================] - 127s 485us/step - loss: 5.3430 - acc: 0.1305 - val_loss: 8.5248 - val_acc: 0.0669\n",
      "Epoch 31/300\n",
      "262555/262555 [==============================] - 125s 477us/step - loss: 5.3296 - acc: 0.1324 - val_loss: 8.5933 - val_acc: 0.0640\n",
      "Epoch 32/300\n",
      "262555/262555 [==============================] - 129s 490us/step - loss: 5.3392 - acc: 0.1319 - val_loss: 8.7113 - val_acc: 0.0615\n",
      "Epoch 33/300\n",
      "262555/262555 [==============================] - 122s 463us/step - loss: 5.3158 - acc: 0.1328 - val_loss: 8.5770 - val_acc: 0.0679\n",
      "Epoch 34/300\n",
      "262555/262555 [==============================] - 122s 464us/step - loss: 5.2984 - acc: 0.1351 - val_loss: 8.5338 - val_acc: 0.0698\n",
      "Epoch 35/300\n",
      "262555/262555 [==============================] - 123s 469us/step - loss: 5.2808 - acc: 0.1361 - val_loss: 8.6027 - val_acc: 0.0665\n",
      "Epoch 36/300\n",
      "262555/262555 [==============================] - 121s 461us/step - loss: 5.2881 - acc: 0.1358 - val_loss: 8.6269 - val_acc: 0.0674\n",
      "Epoch 37/300\n",
      "262555/262555 [==============================] - 123s 468us/step - loss: 5.2813 - acc: 0.1354 - val_loss: 8.8166 - val_acc: 0.0634\n",
      "Epoch 38/300\n",
      "262555/262555 [==============================] - 120s 458us/step - loss: 5.2579 - acc: 0.1374 - val_loss: 8.5183 - val_acc: 0.0734\n",
      "Epoch 39/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 5.2565 - acc: 0.1378 - val_loss: 8.5846 - val_acc: 0.0711\n",
      "Epoch 40/300\n",
      "262555/262555 [==============================] - 122s 463us/step - loss: 5.2483 - acc: 0.1393 - val_loss: 8.7252 - val_acc: 0.0642\n",
      "Epoch 41/300\n",
      "262555/262555 [==============================] - 125s 475us/step - loss: 5.2491 - acc: 0.1381 - val_loss: 8.6641 - val_acc: 0.0652\n",
      "Epoch 42/300\n",
      "262555/262555 [==============================] - 130s 494us/step - loss: 5.2209 - acc: 0.1410 - val_loss: 8.6006 - val_acc: 0.0692\n",
      "Epoch 43/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 5.2239 - acc: 0.1414 - val_loss: 8.6402 - val_acc: 0.0735\n",
      "Epoch 44/300\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 5.2105 - acc: 0.1422 - val_loss: 8.6400 - val_acc: 0.0679: 0s - loss: 5.2086 - acc: 0\n",
      "Epoch 45/300\n",
      "262555/262555 [==============================] - 122s 463us/step - loss: 5.1883 - acc: 0.1431 - val_loss: 8.7378 - val_acc: 0.0678\n",
      "Epoch 46/300\n",
      "262555/262555 [==============================] - 125s 475us/step - loss: 5.1964 - acc: 0.1430 - val_loss: 8.6325 - val_acc: 0.0694\n",
      "Epoch 47/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 5.1881 - acc: 0.1439 - val_loss: 8.8628 - val_acc: 0.0620\n",
      "Epoch 48/300\n",
      "262555/262555 [==============================] - 121s 461us/step - loss: 5.1997 - acc: 0.1433 - val_loss: 8.8304 - val_acc: 0.0632: \n",
      "Epoch 49/300\n",
      "262555/262555 [==============================] - 123s 470us/step - loss: 5.1789 - acc: 0.1445 - val_loss: 9.0561 - val_acc: 0.0541\n",
      "Epoch 50/300\n",
      "262555/262555 [==============================] - 123s 467us/step - loss: 5.1810 - acc: 0.1447 - val_loss: 8.7640 - val_acc: 0.0621\n",
      "Epoch 51/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 5.1598 - acc: 0.1458 - val_loss: 8.8886 - val_acc: 0.0646\n",
      "Epoch 52/300\n",
      "262555/262555 [==============================] - 127s 483us/step - loss: 5.1414 - acc: 0.1481 - val_loss: 8.8345 - val_acc: 0.0595\n",
      "Epoch 53/300\n",
      "262555/262555 [==============================] - 124s 472us/step - loss: 5.1453 - acc: 0.1487 - val_loss: 8.7434 - val_acc: 0.0645\n",
      "Epoch 54/300\n",
      "262555/262555 [==============================] - 127s 482us/step - loss: 5.1477 - acc: 0.1473 - val_loss: 8.8345 - val_acc: 0.0649\n",
      "Epoch 55/300\n",
      "262555/262555 [==============================] - 120s 456us/step - loss: 5.1285 - acc: 0.1494 - val_loss: 8.9292 - val_acc: 0.0655\n",
      "Epoch 56/300\n",
      "262555/262555 [==============================] - 130s 494us/step - loss: 5.1300 - acc: 0.1493 - val_loss: 9.0193 - val_acc: 0.0603\n",
      "Epoch 57/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262555/262555 [==============================] - 127s 482us/step - loss: 5.1166 - acc: 0.1499 - val_loss: 8.9225 - val_acc: 0.0601\n",
      "Epoch 58/300\n",
      "262555/262555 [==============================] - 126s 480us/step - loss: 5.0975 - acc: 0.1524 - val_loss: 8.9172 - val_acc: 0.0645\n",
      "Epoch 59/300\n",
      "262555/262555 [==============================] - 127s 484us/step - loss: 5.1033 - acc: 0.1511 - val_loss: 8.7876 - val_acc: 0.0692\n",
      "Epoch 60/300\n",
      "262555/262555 [==============================] - 131s 498us/step - loss: 5.1040 - acc: 0.1517 - val_loss: 8.9001 - val_acc: 0.0620\n",
      "Epoch 61/300\n",
      "262555/262555 [==============================] - 129s 491us/step - loss: 5.1066 - acc: 0.1510 - val_loss: 9.0192 - val_acc: 0.0599\n",
      "Epoch 62/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 5.1035 - acc: 0.1519 - val_loss: 9.0566 - val_acc: 0.0546\n",
      "Epoch 63/300\n",
      "262555/262555 [==============================] - 126s 479us/step - loss: 5.1163 - acc: 0.1501 - val_loss: 8.7620 - val_acc: 0.0668\n",
      "Epoch 64/300\n",
      "262555/262555 [==============================] - 122s 463us/step - loss: 5.1136 - acc: 0.1511 - val_loss: 8.8124 - val_acc: 0.0659\n",
      "Epoch 65/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 5.0740 - acc: 0.1532 - val_loss: 9.0017 - val_acc: 0.0632\n",
      "Epoch 66/300\n",
      "262555/262555 [==============================] - 122s 466us/step - loss: 5.0625 - acc: 0.1550 - val_loss: 8.9742 - val_acc: 0.0690\n",
      "Epoch 67/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 5.0622 - acc: 0.1549 - val_loss: 9.0622 - val_acc: 0.0577\n",
      "Epoch 68/300\n",
      "262555/262555 [==============================] - 123s 469us/step - loss: 5.0549 - acc: 0.1567 - val_loss: 8.8451 - val_acc: 0.0703\n",
      "Epoch 69/300\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 5.0500 - acc: 0.1572 - val_loss: 8.8167 - val_acc: 0.0683\n",
      "Epoch 70/300\n",
      "262555/262555 [==============================] - 128s 489us/step - loss: 5.0543 - acc: 0.1568 - val_loss: 8.9345 - val_acc: 0.0638\n",
      "Epoch 71/300\n",
      "262555/262555 [==============================] - 128s 486us/step - loss: 5.0471 - acc: 0.1573 - val_loss: 8.9415 - val_acc: 0.0645\n",
      "Epoch 72/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 5.0296 - acc: 0.1587 - val_loss: 8.8842 - val_acc: 0.0690\n",
      "Epoch 73/300\n",
      "262555/262555 [==============================] - 128s 486us/step - loss: 5.0525 - acc: 0.1569 - val_loss: 8.9173 - val_acc: 0.0659\n",
      "Epoch 74/300\n",
      "262555/262555 [==============================] - 120s 458us/step - loss: 5.0367 - acc: 0.1586 - val_loss: 8.9379 - val_acc: 0.0633 - \n",
      "Epoch 75/300\n",
      "262555/262555 [==============================] - 122s 466us/step - loss: 5.0188 - acc: 0.1599 - val_loss: 8.8400 - val_acc: 0.0638\n",
      "Epoch 76/300\n",
      "262555/262555 [==============================] - 119s 452us/step - loss: 5.0209 - acc: 0.1595 - val_loss: 8.9618 - val_acc: 0.0690\n",
      "Epoch 77/300\n",
      "262555/262555 [==============================] - 119s 453us/step - loss: 5.0052 - acc: 0.1617 - val_loss: 8.8961 - val_acc: 0.0676\n",
      "Epoch 78/300\n",
      "262555/262555 [==============================] - 126s 478us/step - loss: 4.9948 - acc: 0.1623 - val_loss: 9.1181 - val_acc: 0.0624\n",
      "Epoch 79/300\n",
      "262555/262555 [==============================] - 123s 468us/step - loss: 5.0168 - acc: 0.1618 - val_loss: 9.0026 - val_acc: 0.0690\n",
      "Epoch 80/300\n",
      "262555/262555 [==============================] - 125s 475us/step - loss: 4.9913 - acc: 0.1628 - val_loss: 9.1512 - val_acc: 0.0641\n",
      "Epoch 81/300\n",
      "262555/262555 [==============================] - 123s 467us/step - loss: 4.9811 - acc: 0.1633 - val_loss: 9.2239 - val_acc: 0.0624\n",
      "Epoch 82/300\n",
      "262555/262555 [==============================] - 122s 464us/step - loss: 4.9778 - acc: 0.1643 - val_loss: 9.1062 - val_acc: 0.0605\n",
      "Epoch 83/300\n",
      "262555/262555 [==============================] - 135s 516us/step - loss: 4.9841 - acc: 0.1639 - val_loss: 8.9489 - val_acc: 0.0682\n",
      "Epoch 84/300\n",
      "262555/262555 [==============================] - 125s 478us/step - loss: 4.9723 - acc: 0.1639 - val_loss: 8.9522 - val_acc: 0.0677\n",
      "Epoch 85/300\n",
      "262555/262555 [==============================] - 125s 478us/step - loss: 4.9581 - acc: 0.1659 - val_loss: 9.2798 - val_acc: 0.0578\n",
      "Epoch 86/300\n",
      "262555/262555 [==============================] - 127s 482us/step - loss: 4.9810 - acc: 0.1633 - val_loss: 9.0936 - val_acc: 0.0647\n",
      "Epoch 87/300\n",
      "262555/262555 [==============================] - 128s 486us/step - loss: 4.9661 - acc: 0.1649 - val_loss: 9.3020 - val_acc: 0.0538\n",
      "Epoch 88/300\n",
      "262555/262555 [==============================] - 121s 461us/step - loss: 4.9860 - acc: 0.1632 - val_loss: 8.9999 - val_acc: 0.0691\n",
      "Epoch 89/300\n",
      "262555/262555 [==============================] - 132s 503us/step - loss: 4.9872 - acc: 0.1624 - val_loss: 9.1539 - val_acc: 0.0591\n",
      "Epoch 90/300\n",
      "262555/262555 [==============================] - 125s 476us/step - loss: 4.9925 - acc: 0.1637 - val_loss: 9.0418 - val_acc: 0.0670\n",
      "Epoch 91/300\n",
      "262555/262555 [==============================] - 125s 476us/step - loss: 4.9777 - acc: 0.1633 - val_loss: 9.0388 - val_acc: 0.0617\n",
      "Epoch 92/300\n",
      "262555/262555 [==============================] - 126s 480us/step - loss: 4.9644 - acc: 0.1653 - val_loss: 9.2754 - val_acc: 0.0579\n",
      "Epoch 93/300\n",
      "262555/262555 [==============================] - 124s 473us/step - loss: 4.9744 - acc: 0.1648 - val_loss: 9.3882 - val_acc: 0.0570s - loss: \n",
      "Epoch 94/300\n",
      "262555/262555 [==============================] - 120s 457us/step - loss: 4.9626 - acc: 0.1648 - val_loss: 9.2843 - val_acc: 0.0563\n",
      "Epoch 95/300\n",
      "262555/262555 [==============================] - 122s 466us/step - loss: 4.9617 - acc: 0.1662 - val_loss: 9.1238 - val_acc: 0.0634\n",
      "Epoch 96/300\n",
      "262555/262555 [==============================] - 127s 484us/step - loss: 4.9490 - acc: 0.1659 - val_loss: 9.2250 - val_acc: 0.0602- acc: 0.165\n",
      "Epoch 97/300\n",
      "262555/262555 [==============================] - 129s 490us/step - loss: 4.9396 - acc: 0.1686 - val_loss: 9.2605 - val_acc: 0.0600\n",
      "Epoch 98/300\n",
      "262555/262555 [==============================] - 131s 498us/step - loss: 4.9429 - acc: 0.1682 - val_loss: 9.1735 - val_acc: 0.0666\n",
      "Epoch 99/300\n",
      "262555/262555 [==============================] - 126s 478us/step - loss: 4.9230 - acc: 0.1689 - val_loss: 8.9476 - val_acc: 0.0691\n",
      "Epoch 100/300\n",
      "262555/262555 [==============================] - 134s 509us/step - loss: 4.9162 - acc: 0.1698 - val_loss: 9.1804 - val_acc: 0.0603\n",
      "Epoch 101/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 4.9102 - acc: 0.1713 - val_loss: 9.1590 - val_acc: 0.0677\n",
      "Epoch 102/300\n",
      "262555/262555 [==============================] - 126s 479us/step - loss: 4.9014 - acc: 0.1713 - val_loss: 9.1733 - val_acc: 0.0642\n",
      "Epoch 103/300\n",
      "262555/262555 [==============================] - 121s 459us/step - loss: 4.8948 - acc: 0.1722 - val_loss: 9.1786 - val_acc: 0.0646\n",
      "Epoch 104/300\n",
      "262555/262555 [==============================] - 125s 475us/step - loss: 4.8977 - acc: 0.1715 - val_loss: 9.2452 - val_acc: 0.0653\n",
      "Epoch 105/300\n",
      "262555/262555 [==============================] - 120s 459us/step - loss: 4.9157 - acc: 0.1705 - val_loss: 9.0321 - val_acc: 0.0662\n",
      "Epoch 106/300\n",
      "262555/262555 [==============================] - 127s 484us/step - loss: 4.9205 - acc: 0.1694 - val_loss: 9.3110 - val_acc: 0.0628\n",
      "Epoch 107/300\n",
      "262555/262555 [==============================] - 122s 464us/step - loss: 4.9081 - acc: 0.1712 - val_loss: 9.2140 - val_acc: 0.0626\n",
      "Epoch 108/300\n",
      "262555/262555 [==============================] - 125s 477us/step - loss: 4.9330 - acc: 0.1680 - val_loss: 9.2659 - val_acc: 0.0588\n",
      "Epoch 109/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 4.9066 - acc: 0.1714 - val_loss: 9.2837 - val_acc: 0.0632\n",
      "Epoch 110/300\n",
      "262555/262555 [==============================] - 132s 504us/step - loss: 4.8837 - acc: 0.1730 - val_loss: 9.2561 - val_acc: 0.0614\n",
      "Epoch 111/300\n",
      "262555/262555 [==============================] - 127s 482us/step - loss: 4.8847 - acc: 0.1735 - val_loss: 9.1147 - val_acc: 0.0663\n",
      "Epoch 112/300\n",
      "262555/262555 [==============================] - 121s 460us/step - loss: 4.8896 - acc: 0.1727 - val_loss: 9.2102 - val_acc: 0.0673\n",
      "Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262555/262555 [==============================] - 121s 461us/step - loss: 4.8773 - acc: 0.1748 - val_loss: 9.1138 - val_acc: 0.0641\n",
      "Epoch 114/300\n",
      "262555/262555 [==============================] - 124s 473us/step - loss: 4.8687 - acc: 0.1745 - val_loss: 9.2918 - val_acc: 0.0626\n",
      "Epoch 115/300\n",
      "262555/262555 [==============================] - 121s 460us/step - loss: 4.8764 - acc: 0.1730 - val_loss: 9.1799 - val_acc: 0.0678E\n",
      "Epoch 116/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 4.8697 - acc: 0.1751 - val_loss: 9.1246 - val_acc: 0.0677\n",
      "Epoch 117/300\n",
      "262555/262555 [==============================] - 122s 466us/step - loss: 4.8609 - acc: 0.1760 - val_loss: 9.0827 - val_acc: 0.0660\n",
      "Epoch 118/300\n",
      "262555/262555 [==============================] - 132s 504us/step - loss: 4.8509 - acc: 0.1767 - val_loss: 9.1891 - val_acc: 0.0660\n",
      "Epoch 119/300\n",
      "262555/262555 [==============================] - 122s 466us/step - loss: 4.8616 - acc: 0.1760 - val_loss: 9.2057 - val_acc: 0.0700\n",
      "Epoch 120/300\n",
      "262555/262555 [==============================] - 122s 466us/step - loss: 4.8535 - acc: 0.1770 - val_loss: 9.2449 - val_acc: 0.0656\n",
      "Epoch 121/300\n",
      "262555/262555 [==============================] - 121s 463us/step - loss: 4.8515 - acc: 0.1775 - val_loss: 9.2765 - val_acc: 0.0614\n",
      "Epoch 122/300\n",
      "262555/262555 [==============================] - 123s 470us/step - loss: 4.8724 - acc: 0.1752 - val_loss: 9.1573 - val_acc: 0.0661\n",
      "Epoch 123/300\n",
      "262555/262555 [==============================] - 127s 485us/step - loss: 4.8426 - acc: 0.1783 - val_loss: 9.3525 - val_acc: 0.0633\n",
      "Epoch 124/300\n",
      "262555/262555 [==============================] - 123s 470us/step - loss: 4.8351 - acc: 0.1784 - val_loss: 9.2873 - val_acc: 0.0650\n",
      "Epoch 125/300\n",
      "262555/262555 [==============================] - 123s 467us/step - loss: 4.8380 - acc: 0.1777 - val_loss: 9.2467 - val_acc: 0.0651\n",
      "Epoch 126/300\n",
      "262555/262555 [==============================] - 127s 485us/step - loss: 4.8438 - acc: 0.1773 - val_loss: 9.4244 - val_acc: 0.0552\n",
      "Epoch 127/300\n",
      "262555/262555 [==============================] - 131s 500us/step - loss: 4.8702 - acc: 0.1748 - val_loss: 9.2533 - val_acc: 0.0651\n",
      "Epoch 128/300\n",
      "262555/262555 [==============================] - 130s 494us/step - loss: 4.8341 - acc: 0.1792 - val_loss: 9.2112 - val_acc: 0.0648\n",
      "Epoch 129/300\n",
      "262555/262555 [==============================] - 123s 469us/step - loss: 4.8325 - acc: 0.1790 - val_loss: 9.2399 - val_acc: 0.0668\n",
      "Epoch 130/300\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 4.8349 - acc: 0.1789 - val_loss: 9.2156 - val_acc: 0.0691\n",
      "Epoch 131/300\n",
      "262555/262555 [==============================] - 125s 477us/step - loss: 4.8105 - acc: 0.1807 - val_loss: 9.3069 - val_acc: 0.0637\n",
      "Epoch 132/300\n",
      "262555/262555 [==============================] - 119s 453us/step - loss: 4.8173 - acc: 0.1807 - val_loss: 9.2794 - val_acc: 0.0639 loss: 4.8170 - acc: 0.\n",
      "Epoch 133/300\n",
      "262555/262555 [==============================] - 124s 472us/step - loss: 4.8196 - acc: 0.1807 - val_loss: 9.3427 - val_acc: 0.0622 - ETA: 11s - loss: 4.80 - ETA: 8s - loss: 4.8058 - acc: 0.\n",
      "Epoch 134/300\n",
      "262555/262555 [==============================] - 128s 488us/step - loss: 4.8086 - acc: 0.1814 - val_loss: 9.1466 - val_acc: 0.0691\n",
      "Epoch 135/300\n",
      "262555/262555 [==============================] - 124s 474us/step - loss: 4.7879 - acc: 0.1841 - val_loss: 9.1510 - val_acc: 0.0676\n",
      "Epoch 136/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 4.8122 - acc: 0.1806 - val_loss: 9.2354 - val_acc: 0.0625\n",
      "Epoch 137/300\n",
      "262555/262555 [==============================] - 121s 463us/step - loss: 4.8223 - acc: 0.1804 - val_loss: 9.1422 - val_acc: 0.0668\n",
      "Epoch 138/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 4.7985 - acc: 0.1821 - val_loss: 9.3764 - val_acc: 0.0637\n",
      "Epoch 139/300\n",
      "262555/262555 [==============================] - 123s 469us/step - loss: 4.8172 - acc: 0.1808 - val_loss: 9.1962 - val_acc: 0.0626\n",
      "Epoch 140/300\n",
      "262555/262555 [==============================] - 123s 467us/step - loss: 4.8073 - acc: 0.1821 - val_loss: 9.4249 - val_acc: 0.0590\n",
      "Epoch 141/300\n",
      "262555/262555 [==============================] - 121s 460us/step - loss: 4.8052 - acc: 0.1813 - val_loss: 9.3654 - val_acc: 0.0654\n",
      "Epoch 142/300\n",
      "262555/262555 [==============================] - 121s 459us/step - loss: 4.8073 - acc: 0.1812 - val_loss: 9.2558 - val_acc: 0.0677\n",
      "Epoch 143/300\n",
      "262555/262555 [==============================] - 128s 487us/step - loss: 4.8060 - acc: 0.1819 - val_loss: 9.0968 - val_acc: 0.0692\n",
      "Epoch 144/300\n",
      "262555/262555 [==============================] - 124s 473us/step - loss: 4.7832 - acc: 0.1828 - val_loss: 9.3249 - val_acc: 0.0654\n",
      "Epoch 145/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 4.7690 - acc: 0.1861 - val_loss: 9.2710 - val_acc: 0.0670\n",
      "Epoch 146/300\n",
      "262555/262555 [==============================] - 123s 470us/step - loss: 4.7999 - acc: 0.1831 - val_loss: 9.4871 - val_acc: 0.0593\n",
      "Epoch 147/300\n",
      "262555/262555 [==============================] - 130s 497us/step - loss: 4.7743 - acc: 0.1851 - val_loss: 9.3291 - val_acc: 0.0619\n",
      "Epoch 148/300\n",
      "262555/262555 [==============================] - 127s 484us/step - loss: 4.8015 - acc: 0.1824 - val_loss: 9.5648 - val_acc: 0.0578\n",
      "Epoch 149/300\n",
      "262555/262555 [==============================] - 138s 527us/step - loss: 4.7909 - acc: 0.1835 - val_loss: 9.2672 - val_acc: 0.0658\n",
      "Epoch 150/300\n",
      "262555/262555 [==============================] - 129s 490us/step - loss: 4.7949 - acc: 0.1841 - val_loss: 9.4151 - val_acc: 0.0652\n",
      "Epoch 151/300\n",
      "262555/262555 [==============================] - 122s 464us/step - loss: 4.7810 - acc: 0.1842 - val_loss: 9.3969 - val_acc: 0.0674\n",
      "Epoch 152/300\n",
      "262555/262555 [==============================] - 123s 467us/step - loss: 4.8106 - acc: 0.1817 - val_loss: 9.2999 - val_acc: 0.0651\n",
      "Epoch 153/300\n",
      "262555/262555 [==============================] - 120s 456us/step - loss: 4.7758 - acc: 0.1851 - val_loss: 9.4398 - val_acc: 0.060344 - a - ETA: 0s - loss: 4.7753 - acc: 0\n",
      "Epoch 154/300\n",
      "262555/262555 [==============================] - 123s 467us/step - loss: 4.7894 - acc: 0.1839 - val_loss: 9.3640 - val_acc: 0.0668\n",
      "Epoch 155/300\n",
      "262555/262555 [==============================] - 128s 486us/step - loss: 4.7667 - acc: 0.1853 - val_loss: 9.4755 - val_acc: 0.0607\n",
      "Epoch 156/300\n",
      "262555/262555 [==============================] - 128s 489us/step - loss: 4.7838 - acc: 0.1846 - val_loss: 9.4396 - val_acc: 0.0624\n",
      "Epoch 157/300\n",
      "262555/262555 [==============================] - 124s 474us/step - loss: 4.7765 - acc: 0.1852 - val_loss: 9.2941 - val_acc: 0.0689\n",
      "Epoch 158/300\n",
      "262555/262555 [==============================] - 128s 487us/step - loss: 4.7683 - acc: 0.1858 - val_loss: 9.3485 - val_acc: 0.0629\n",
      "Epoch 159/300\n",
      "262555/262555 [==============================] - 126s 479us/step - loss: 4.7748 - acc: 0.1858 - val_loss: 9.4123 - val_acc: 0.0626\n",
      "Epoch 160/300\n",
      "262555/262555 [==============================] - 123s 468us/step - loss: 4.7986 - acc: 0.1830 - val_loss: 9.3176 - val_acc: 0.0623\n",
      "Epoch 161/300\n",
      "262555/262555 [==============================] - 121s 463us/step - loss: 4.7606 - acc: 0.1867 - val_loss: 9.4481 - val_acc: 0.0636\n",
      "Epoch 162/300\n",
      "262555/262555 [==============================] - 125s 476us/step - loss: 4.7679 - acc: 0.1873 - val_loss: 9.3557 - val_acc: 0.0574\n",
      "Epoch 163/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 4.7830 - acc: 0.1845 - val_loss: 9.3381 - val_acc: 0.0695\n",
      "Epoch 164/300\n",
      "262555/262555 [==============================] - 125s 478us/step - loss: 4.7458 - acc: 0.1877 - val_loss: 9.5255 - val_acc: 0.0612\n",
      "Epoch 165/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 4.7486 - acc: 0.1876 - val_loss: 9.3632 - val_acc: 0.0635\n",
      "Epoch 166/300\n",
      "262555/262555 [==============================] - 119s 455us/step - loss: 4.7776 - acc: 0.1854 - val_loss: 9.6505 - val_acc: 0.0530\n",
      "Epoch 167/300\n",
      "262555/262555 [==============================] - 126s 478us/step - loss: 4.7850 - acc: 0.1855 - val_loss: 9.4145 - val_acc: 0.0632\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262555/262555 [==============================] - 122s 463us/step - loss: 4.7402 - acc: 0.1885 - val_loss: 9.3979 - val_acc: 0.0646\n",
      "Epoch 169/300\n",
      "262555/262555 [==============================] - 127s 483us/step - loss: 4.7383 - acc: 0.1885 - val_loss: 9.3646 - val_acc: 0.0672\n",
      "Epoch 170/300\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 4.7640 - acc: 0.1862 - val_loss: 9.3424 - val_acc: 0.0643\n",
      "Epoch 171/300\n",
      "262555/262555 [==============================] - 125s 478us/step - loss: 4.7181 - acc: 0.1905 - val_loss: 9.5788 - val_acc: 0.0625\n",
      "Epoch 172/300\n",
      "262555/262555 [==============================] - 121s 461us/step - loss: 4.7333 - acc: 0.1902 - val_loss: 9.4773 - val_acc: 0.0622\n",
      "Epoch 173/300\n",
      "262555/262555 [==============================] - 123s 469us/step - loss: 4.7253 - acc: 0.1899 - val_loss: 9.4509 - val_acc: 0.0661\n",
      "Epoch 174/300\n",
      "262555/262555 [==============================] - 125s 475us/step - loss: 4.7328 - acc: 0.1897 - val_loss: 9.3645 - val_acc: 0.0666\n",
      "Epoch 175/300\n",
      "262555/262555 [==============================] - 122s 465us/step - loss: 4.7349 - acc: 0.1901 - val_loss: 9.3764 - val_acc: 0.0666\n",
      "Epoch 176/300\n",
      "262555/262555 [==============================] - 129s 492us/step - loss: 4.7176 - acc: 0.1914 - val_loss: 9.3611 - val_acc: 0.0690\n",
      "Epoch 177/300\n",
      "262555/262555 [==============================] - 138s 527us/step - loss: 4.7145 - acc: 0.1914 - val_loss: 9.3611 - val_acc: 0.0686\n",
      "Epoch 178/300\n",
      "262555/262555 [==============================] - 122s 463us/step - loss: 4.7429 - acc: 0.1887 - val_loss: 9.4297 - val_acc: 0.0673\n",
      "Epoch 179/300\n",
      "262555/262555 [==============================] - 125s 477us/step - loss: 4.7394 - acc: 0.1894 - val_loss: 9.6061 - val_acc: 0.0550\n",
      "Epoch 180/300\n",
      "262555/262555 [==============================] - 120s 457us/step - loss: 4.7421 - acc: 0.1896 - val_loss: 9.4436 - val_acc: 0.0630\n",
      "Epoch 181/300\n",
      "262555/262555 [==============================] - 126s 479us/step - loss: 4.7294 - acc: 0.1898 - val_loss: 9.3878 - val_acc: 0.0629\n",
      "Epoch 182/300\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 4.7175 - acc: 0.1925 - val_loss: 9.3101 - val_acc: 0.0641\n",
      "Epoch 183/300\n",
      "262555/262555 [==============================] - 125s 476us/step - loss: 4.7309 - acc: 0.1904 - val_loss: 9.4430 - val_acc: 0.0644\n",
      "Epoch 184/300\n",
      "262555/262555 [==============================] - 122s 463us/step - loss: 4.7037 - acc: 0.1929 - val_loss: 9.4460 - val_acc: 0.0645\n",
      "Epoch 185/300\n",
      "262555/262555 [==============================] - 127s 483us/step - loss: 4.7309 - acc: 0.1905 - val_loss: 9.3313 - val_acc: 0.0716\n",
      "Epoch 186/300\n",
      "262555/262555 [==============================] - 127s 485us/step - loss: 4.7153 - acc: 0.1913 - val_loss: 9.4388 - val_acc: 0.0648\n",
      "Epoch 187/300\n",
      "262555/262555 [==============================] - 123s 470us/step - loss: 4.7081 - acc: 0.1928 - val_loss: 9.4686 - val_acc: 0.0667\n",
      "Epoch 188/300\n",
      "262555/262555 [==============================] - 123s 469us/step - loss: 4.7121 - acc: 0.1923 - val_loss: 9.4476 - val_acc: 0.0608\n",
      "Epoch 189/300\n",
      "262555/262555 [==============================] - 125s 477us/step - loss: 4.7481 - acc: 0.1884 - val_loss: 9.5428 - val_acc: 0.0630\n",
      "Epoch 190/300\n",
      "262555/262555 [==============================] - 121s 461us/step - loss: 4.7055 - acc: 0.1930 - val_loss: 9.3542 - val_acc: 0.0685\n",
      "Epoch 191/300\n",
      "262555/262555 [==============================] - 124s 471us/step - loss: 4.7214 - acc: 0.1915 - val_loss: 9.4921 - val_acc: 0.0649\n",
      "Epoch 192/300\n",
      "262555/262555 [==============================] - 122s 464us/step - loss: 4.7125 - acc: 0.1916 - val_loss: 9.4210 - val_acc: 0.0680\n",
      "Epoch 193/300\n",
      "262555/262555 [==============================] - 126s 481us/step - loss: 4.7419 - acc: 0.1894 - val_loss: 9.4761 - val_acc: 0.0666\n",
      "Epoch 194/300\n",
      "262555/262555 [==============================] - 125s 474us/step - loss: 4.7180 - acc: 0.1925 - val_loss: 9.4858 - val_acc: 0.0571\n",
      "Epoch 195/300\n",
      "262555/262555 [==============================] - 127s 482us/step - loss: 4.7080 - acc: 0.1929 - val_loss: 9.4485 - val_acc: 0.0646\n",
      "Epoch 196/300\n",
      "262555/262555 [==============================] - 130s 496us/step - loss: 4.7110 - acc: 0.1933 - val_loss: 9.4073 - val_acc: 0.0627\n",
      "Epoch 197/300\n",
      "262555/262555 [==============================] - 139s 528us/step - loss: 4.7012 - acc: 0.1929 - val_loss: 9.5520 - val_acc: 0.0584\n",
      "Epoch 198/300\n",
      "262555/262555 [==============================] - 128s 486us/step - loss: 4.7018 - acc: 0.1939 - val_loss: 9.6426 - val_acc: 0.0627\n",
      "Epoch 199/300\n",
      "262555/262555 [==============================] - 128s 488us/step - loss: 4.7481 - acc: 0.1887 - val_loss: 9.5599 - val_acc: 0.0644\n",
      "Epoch 200/300\n",
      "262555/262555 [==============================] - 127s 483us/step - loss: 4.6958 - acc: 0.1936 - val_loss: 9.5258 - val_acc: 0.0628\n",
      "Epoch 201/300\n",
      "262555/262555 [==============================] - 126s 480us/step - loss: 4.6850 - acc: 0.1948 - val_loss: 9.3133 - val_acc: 0.0688s - loss: 4.6819 \n",
      "Epoch 202/300\n",
      "262555/262555 [==============================] - 129s 491us/step - loss: 4.6721 - acc: 0.1971 - val_loss: 9.3435 - val_acc: 0.0687\n",
      "Epoch 203/300\n",
      "262555/262555 [==============================] - 130s 496us/step - loss: 4.7085 - acc: 0.1921 - val_loss: 9.5027 - val_acc: 0.0642\n",
      "Epoch 204/300\n",
      "262555/262555 [==============================] - 130s 495us/step - loss: 4.6981 - acc: 0.1945 - val_loss: 9.4244 - val_acc: 0.0677\n",
      "Epoch 205/300\n",
      "262555/262555 [==============================] - 134s 510us/step - loss: 4.7168 - acc: 0.1922 - val_loss: 9.5493 - val_acc: 0.0616\n",
      "Epoch 206/300\n",
      "262555/262555 [==============================] - 151s 574us/step - loss: 4.6985 - acc: 0.1937 - val_loss: 9.4458 - val_acc: 0.0675\n",
      "Epoch 207/300\n",
      "262555/262555 [==============================] - 120s 458us/step - loss: 4.7023 - acc: 0.1932 - val_loss: 9.4098 - val_acc: 0.0657\n",
      "Epoch 208/300\n",
      "262555/262555 [==============================] - 113s 432us/step - loss: 4.7005 - acc: 0.1947 - val_loss: 9.5937 - val_acc: 0.0614\n",
      "Epoch 209/300\n",
      "262555/262555 [==============================] - 112s 428us/step - loss: 4.6921 - acc: 0.1939 - val_loss: 9.6876 - val_acc: 0.0592\n",
      "Epoch 210/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 4.6814 - acc: 0.1965 - val_loss: 9.4550 - val_acc: 0.0652\n",
      "Epoch 211/300\n",
      "262555/262555 [==============================] - 113s 432us/step - loss: 4.6735 - acc: 0.1963 - val_loss: 9.5774 - val_acc: 0.0565\n",
      "Epoch 212/300\n",
      "262555/262555 [==============================] - 113s 431us/step - loss: 4.6961 - acc: 0.1945 - val_loss: 9.4415 - val_acc: 0.0659\n",
      "Epoch 213/300\n",
      "262555/262555 [==============================] - 112s 427us/step - loss: 4.7063 - acc: 0.1936 - val_loss: 9.3516 - val_acc: 0.0683\n",
      "Epoch 214/300\n",
      "262555/262555 [==============================] - 118s 448us/step - loss: 4.6762 - acc: 0.1962 - val_loss: 9.3341 - val_acc: 0.066268 - ETA: 2s - loss: 4.6729 - acc: 0. - ETA: 2s - loss: 4.6744 - acc: 0 - ETA: 1s - loss: 4.6751 - acc: \n",
      "Epoch 215/300\n",
      "262555/262555 [==============================] - 114s 433us/step - loss: 4.7090 - acc: 0.1936 - val_loss: 9.3500 - val_acc: 0.0693c: 0.19\n",
      "Epoch 216/300\n",
      "262555/262555 [==============================] - 113s 430us/step - loss: 4.7139 - acc: 0.1928 - val_loss: 9.4998 - val_acc: 0.0635\n",
      "Epoch 217/300\n",
      "262555/262555 [==============================] - 130s 496us/step - loss: 4.6891 - acc: 0.1955 - val_loss: 9.7411 - val_acc: 0.0530\n",
      "Epoch 218/300\n",
      "262555/262555 [==============================] - 119s 452us/step - loss: 4.6910 - acc: 0.1953 - val_loss: 9.5039 - val_acc: 0.0637\n",
      "Epoch 219/300\n",
      "262555/262555 [==============================] - 120s 456us/step - loss: 4.6910 - acc: 0.1946 - val_loss: 9.5083 - val_acc: 0.0680\n",
      "Epoch 220/300\n",
      "262555/262555 [==============================] - 121s 462us/step - loss: 4.6589 - acc: 0.1987 - val_loss: 9.4137 - val_acc: 0.0665 4.6559 \n",
      "Epoch 221/300\n",
      "262555/262555 [==============================] - 117s 446us/step - loss: 4.6900 - acc: 0.1946 - val_loss: 9.5914 - val_acc: 0.0614\n",
      "Epoch 222/300\n",
      "262555/262555 [==============================] - 118s 450us/step - loss: 4.6854 - acc: 0.1955 - val_loss: 9.5307 - val_acc: 0.0651\n",
      "Epoch 223/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262555/262555 [==============================] - 115s 440us/step - loss: 4.7026 - acc: 0.1939 - val_loss: 9.5887 - val_acc: 0.0613\n",
      "Epoch 224/300\n",
      "262555/262555 [==============================] - 121s 460us/step - loss: 4.6743 - acc: 0.1964 - val_loss: 9.4713 - val_acc: 0.0614\n",
      "Epoch 225/300\n",
      "262555/262555 [==============================] - 116s 441us/step - loss: 4.6450 - acc: 0.2002 - val_loss: 9.7395 - val_acc: 0.0590\n",
      "Epoch 226/300\n",
      "262555/262555 [==============================] - 118s 450us/step - loss: 4.6676 - acc: 0.1975 - val_loss: 9.6600 - val_acc: 0.0598 - loss: 4.6656 - acc - ETA: 0s - loss: 4.6676 - acc: 0.19 - ETA: 0s - loss: 4.6675 - acc: 0.1\n",
      "Epoch 227/300\n",
      "262555/262555 [==============================] - 117s 444us/step - loss: 4.6681 - acc: 0.1972 - val_loss: 9.8177 - val_acc: 0.05290 - ETA: 9s - loss: 4.6551 - acc: 0. - ETA: 8s - loss: 4.6555 - acc: 0.1\n",
      "Epoch 228/300\n",
      "262555/262555 [==============================] - 117s 445us/step - loss: 4.6897 - acc: 0.1955 - val_loss: 9.4669 - val_acc: 0.0654\n",
      "Epoch 229/300\n",
      "262555/262555 [==============================] - 118s 448us/step - loss: 4.7291 - acc: 0.1914 - val_loss: 9.5663 - val_acc: 0.0627\n",
      "Epoch 230/300\n",
      "262555/262555 [==============================] - 118s 449us/step - loss: 4.6853 - acc: 0.1962 - val_loss: 9.5732 - val_acc: 0.0599\n",
      "Epoch 231/300\n",
      "250500/262555 [===========================>..] - ETA: 5s - loss: 4.6766 - acc: 0.1967"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-08e15a3b92b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_data=(X_test,y_test))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batch_size = 500\n",
    "# epochs = 300\n",
    "# history = model.fit(X_train,y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1,\n",
    "#                     validation_data=(X_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_histt = model.fit(X_train, y_train, epochs=100, batch_size=256, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('Test loss=%s'% scores[0])\n",
    "print('Test accuracy=%s'% scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN_MODEL = \"ANN_MODEL_v4\"\n",
    "# VECTORIZER = \"Vectorizer_v4\"\n",
    "# ENCODER = \"ENCODER_v4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# storeModel(model,ANN_MODEL)\n",
    "# storeModel(vectorizer,VECTORIZER)\n",
    "# storeModel(encoder,ENCODER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# def loadModel(filename):\n",
    "#     loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#     return loaded_model\n",
    "\n",
    "# vectorizer = loadModel(VECTORIZER)\n",
    "# encoder = loadModel(ENCODER)\n",
    "# model = loadModel(ANN_MODEL)\n",
    "# svd = loadModel(SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "    with open(path) as f:\n",
    "        lineList = f.readlines()\n",
    "    return lineList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = readFile('../data/test_tweets_unlabeled.txt')\n",
    "df_test = pd.DataFrame(test_data,columns = ['rtweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = preprocessingDF(df_test)\n",
    "#df_test = dropFeatures(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalizing test data\n",
    "\n",
    "\n",
    "wdCount_predict = df_predict['wdCount'].values #returns a numpy array\n",
    "wdCount_predict = wdCount_predict.reshape(-1, 1)\n",
    "wdCount_scaled_predict = wdCount_scaler.transform(wdCount_predict)\n",
    "df_predict['wdCount'] = pd.DataFrame(wdCount_scaled_predict)\n",
    "\n",
    "\n",
    "\n",
    "chCount_predict = df_predict['chCount'].values #returns a numpy array\n",
    "chCount_predict = chCount_predict.reshape(-1, 1)\n",
    "\n",
    "chCount_scaled_predict = chCount_scaler.transform(chCount_predict)\n",
    "df_predict['chCount'] = pd.DataFrame(chCount_scaled_predict)\n",
    "\n",
    "\n",
    "#df_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_bow_predict = vectorizer.transform(df_predict['tweet'].values)\n",
    "matrix_predict_lowrank = svd.transform(X_bow_predict)\n",
    "\n",
    "df_bow_predict = pd.DataFrame(matrix_predict_lowrank)\n",
    "combDF_predict = pd.concat([df_predict.drop(columns=[\"rtweet\",\"tweet\"]), df_bow_predict], axis=1)\n",
    "combDF_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tfidf_predict = tfidf_vecs.transform(df_predict['tweet'].values)\n",
    "\n",
    "\n",
    "matrix_tfidf_predict_lowrank = svd_tfidf.transform(X_tfidf_predict)\n",
    "\n",
    "df_tfidf_predict = pd.DataFrame(matrix_tfidf_predict_lowrank)\n",
    "combDF_predict_total = pd.concat([combDF_predict, df_tfidf_predict], axis=1)\n",
    "combDF_predict_total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_sentences  = vectorizer.transform(df_test['tweet'])\n",
    "# matrix_test_lowrank = svd.transform(test_sentences)\n",
    "# count_test_df = pd.DataFrame(matrix_test_lowrank)\n",
    "\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combDF_predict_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Concatenate \n",
    "# df2_test = df_test.drop(columns=[\"rtweet\",\"tweet\"])\n",
    "# combined_test_df = pd.concat([df2_test, count_test_df], axis=1)\n",
    "# combined_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testdata = combDF_predict_total.iloc[:, 0:].values\n",
    "\n",
    "y_testdata = model.predict_classes(X_testdata)\n",
    "y_output = encoder.inverse_transform(y_testdata)\n",
    "\n",
    "\n",
    "result = []\n",
    "for i in range(0,len(y_output)):\n",
    "    result.append(y_output[i])\n",
    "print(len(result))\n",
    "resDF = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDF.to_csv(\"../features/unlabelled_prediction_v18.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c whodunnit -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelLSTM = Sequential()\n",
    "# modelLSTM.add(Embedding(MAX_FEATURES, 128))\n",
    "# modelLSTM.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2,\n",
    "#                activation='tanh', return_sequences=True))\n",
    "# modelLSTM.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2, activation='tanh'))\n",
    "# modelLSTM.add(Dense(number_classes, activation='sigmoid'))\n",
    "# modelLSTM.compile(loss='categorical_crossentropy', optimizer = 'rmsprop',\n",
    "#               metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
